{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the iDigBio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "This is data that I gleaned from a raw iDigBio dump for gazetteer input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from flora.pylib.traits import terms as f_terms\n",
    "from tqdm.notebook import tqdm\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52eb3ce8-6ddb-45db-abee-79a818066b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will definitely need to change these directories\n",
    "INPUT_DATA_DIR = Path(\"..\") / \"..\" / \"..\" / \"digi-leap\" / \"gazetteer\" / \"data\"\n",
    "INPUT_DB = INPUT_DATA_DIR / \"gazetteer_04_idigbio_2020-03-30.db\"\n",
    "FIELDS = \"\"\"\n",
    "    locality continent country countryCode county higherGeography island islandGroup\n",
    "    locationRemarks municipality stateProvince waterBody\n",
    "    \"\"\".split()\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"data\" / \"idigbio\"\n",
    "TEMP_DIR = DATA_DIR / \"temp\"\n",
    "DB = DATA_DIR / \"localities.sqlite\"\n",
    "LOCALITIES = DATA_DIR / \"locality_terms.csv\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes\n",
    "\n",
    "# Spacy POS tags to skip\n",
    "POS_ALIAS = \" ADP CCONJ DET NUM PUNCT SCONJ SYM \".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOO_SHORT = 3\n",
    "\n",
    "PUNCT = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "SUBS = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {PUNCT} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading PUNCT\n",
    "    (re.compile(rf\"^( \\s* {PUNCT} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing PUNCT\n",
    "    (re.compile(rf\"( \\s* {PUNCT} \\s* )+ $\", flags=re.X), \" \"),\n",
    "    # Handle contractions & possesives\n",
    "    (re.compile(r\" \\s ( '[st] ) \", flags=re.X), r\"\\1\"),\n",
    "    # Handle abbreviations\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{1,4}) \\s ( \\. ) \", flags=re.X), r\"\\1\\2\"),\n",
    "    # Handle periods\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{5,}) ( \\. ) \", flags=re.X), r\"\\1 \\2\"),\n",
    "    # Remove back slashes\n",
    "    (re.compile(r\"\\\\\", flags=re.X), \"\"),\n",
    "]\n",
    "\n",
    "# Character classes\n",
    "CONTROLS = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "SYMBOLS = \" Sc \".split()  # Currency symbols\n",
    "SEPARATORS = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "REMOVE = CONTROLS + SYMBOLS + SEPARATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, repl in subs:\n",
    "            loc = regexp.sub(repl, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in REMOVE else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        loc = substitute(loc, SUBS)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= TOO_SHORT:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(INPUT_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from gazetteer\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        for rec in tqdm(cxn.execute(\"\"\"select * from gazetteer\"\"\"), total=total):\n",
    "            for field in FIELDS:\n",
    "                if field:\n",
    "                    loc = rec[field]\n",
    "\n",
    "                    norm = normalize_location(loc)\n",
    "                    if norm.loc:\n",
    "                        normals[norm.loc] += norm.add\n",
    "                    errors += norm.error\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b062c7-2558-4a66-9fbf-a82285b55b51",
   "metadata": {},
   "source": [
    "## Get locality words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7110400-c771-41b7-8453-722d70dc5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_RE = re.compile(r\"^ [\\d.Â°/,\\'\\\"+-]+ \", flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9613737e-b870-452e-b682-41c5d6f52f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_proc(limit, offset):\n",
    "    errors = 0\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    taxa_dir = Path(f_terms.__file__).parent\n",
    "    # binomial_terms = taxa_dir / \"binomial_terms.zip\"\n",
    "    monomial_terms = taxa_dir / \"monomial_terms.zip\"\n",
    "\n",
    "    taxa = tu.read_terms([monomial_terms])  # , binomial_terms])\n",
    "    taxa = {t[\"pattern\"] for t in taxa for w in t[\"pattern\"].split()}\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        rows = cxn.execute(\n",
    "            \"\"\"select * from normalized limit ? offset ?\"\"\", (limit, offset)\n",
    "        )\n",
    "\n",
    "        for phrase, hits in rows:\n",
    "            try:\n",
    "                doc = nlp(phrase)\n",
    "            except ValueError:\n",
    "                errors += 1\n",
    "                continue\n",
    "\n",
    "            for token in doc:\n",
    "                if token.pos_ in POS_ALIAS:\n",
    "                    continue\n",
    "\n",
    "                elif token.is_punct or token.is_quote:\n",
    "                    continue\n",
    "\n",
    "                word = NUM_RE.sub(\"\", token.lower_)\n",
    "\n",
    "                if len(word) <= 1:\n",
    "                    continue\n",
    "\n",
    "                if word in taxa:\n",
    "                    continue\n",
    "\n",
    "                all_words[word] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        csv_path = TEMP_DIR / f\"words_{offset}.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f70ed2-46e8-4f61-a256-ce24875aa5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69afd1680be4d768433e770f5ed9953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_words():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from normalized\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    total = sum(1 for _ in range(0, count, limit))\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm(total=total) as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_words_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acea716f-790a-46a4-a243-40fe9d3beb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634c9ebd95a045c2b45e32d39c7ad566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words)=1285747\n",
      "sum(words.values())=190769141\n"
     ]
    }
   ],
   "source": [
    "def write_words():\n",
    "    words = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(TEMP_DIR.glob(\"words_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                word = row[\"pattern\"]\n",
    "                words[word] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(words)=}\")\n",
    "    print(f\"{sum(words.values())=}\")\n",
    "\n",
    "    batch = [{\"pattern\": k, \"hits\": v} for k, v in words.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(DB) as cxn:\n",
    "        df.to_sql(\"words\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    df.to_csv(LOCALITIES, index=False)\n",
    "\n",
    "\n",
    "write_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dcb87-c125-476f-a102-ce00bc524e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
