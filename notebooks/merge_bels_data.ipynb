{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the BELS gazetteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "BELS has a different goal than Digi-Leap BELS is trying to pinpoint latitudes and longitudes from strings and Digi-Leap is parsing strings that may then be fed into BELS. I'm taking the data from BELS and trying to reduce it to a smaller number of patterns that can be fed into spaCy rule parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from flora.pylib.traits import terms as f_terms\n",
    "from tqdm.notebook import tqdm\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c4f7ce-becf-469e-85a6-5fd7475cf743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47d021-c349-473e-93ff-15a98ad32130",
   "metadata": {},
   "source": [
    "## BELS localities given to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bd20a5-92c9-463d-8b9a-285f8bb2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELS = DATA_DIR / \"bels\"\n",
    "BELS_DB = BELS / \"localities.sqlite\"\n",
    "BELS_ORI = BELS / \"original\"\n",
    "BELS_TEMP = BELS / \"temp\"\n",
    "\n",
    "LOCALITIES = BELS / \"locality_terms.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dba32d-0660-4804-a559-dd8c43abf92d",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b191f7b4-31cb-4cef-bd68-aaa77fed4588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    paths = sorted(BELS_ORI.glob(\"*.csv.gz\"))\n",
    "\n",
    "    if_exists = \"replace\"\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        for path in tqdm(paths):\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            locs = df[\"v_locality\"]\n",
    "            locs.to_sql(\"raw\", cxn, index=False, if_exists=if_exists)\n",
    "\n",
    "            if_exists = \"append\"\n",
    "\n",
    "\n",
    "# get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TOO_SHORT = 3\n",
    "\n",
    "PUNCT = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "SUBS = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {PUNCT} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading PUNCT\n",
    "    (re.compile(rf\"^( \\s* {PUNCT} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing PUNCT\n",
    "    (re.compile(rf\"( \\s* {PUNCT} \\s* )+ $\", flags=re.X), \" \"),\n",
    "    # Handle contractions & possesives\n",
    "    (re.compile(r\" \\s ( '[st] ) \", flags=re.X), r\"\\1\"),\n",
    "    # Handle abbreviations\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{1,4}) \\s ( \\. ) \", flags=re.X), r\"\\1\\2\"),\n",
    "    # Handle periods\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{5,}) ( \\. ) \", flags=re.X), r\"\\1 \\2\"),\n",
    "    # Remove back slashes\n",
    "    (re.compile(r\"\\\\\", flags=re.X), \"\"),\n",
    "]\n",
    "\n",
    "# Character classes\n",
    "CONTROLS = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "SYMBOLS = \" Sc \".split()  # Currency symbols\n",
    "SEPARATORS = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "REMOVE = CONTROLS + SYMBOLS + SEPARATORS\n",
    "\n",
    "# Spacy POS tags\n",
    "POS_ALIAS = \" CCONJ DET NUM SCONJ \".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, repl in subs:\n",
    "            loc = regexp.sub(repl, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in REMOVE else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        loc = substitute(loc, SUBS)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= TOO_SHORT:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from raw\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        for loc in tqdm(cxn.execute(\"\"\"select * from raw\"\"\"), total=total):\n",
    "            loc = loc[\"v_locality\"]\n",
    "\n",
    "            norm = normalize_location(loc)\n",
    "            if norm.loc:\n",
    "                normals[norm.loc] += norm.add\n",
    "            errors += norm.error\n",
    "\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff63a5-5d25-4681-bf7e-24b16f901829",
   "metadata": {},
   "source": [
    "## Alias localities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9176fac-d1d2-4d36-8348-39dd21a9b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get words that get replaced in the BELS noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b45189-dba6-4f87-a8eb-52d879bd55fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The are sets of common terms and patterns stored in CSV files that are used to categorize word or phrase types like colors or dates. We're going to use them to replace terms in the BELS noun phrases with hypernyms. For instance:\n",
    "\n",
    "- Replace `12 North Main Street` with `<num> <dir> main street`\n",
    "\n",
    "The hope is to cut down on the total number of patterns stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72d3b3e-713d-4778-a4c4-853d7cfd1f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    term_dir = Path(t_terms.__file__).parent\n",
    "    flora_dir = Path(f_terms.__file__).parent\n",
    "    tokens = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get units\n",
    "    path = [\n",
    "        term_dir / \"unit_distance_terms.csv\",\n",
    "        term_dir / \"unit_length_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # Skip anything smaller than a foot\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if float(t[\"factor_cm\"]) > 30.0}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get other tokens\n",
    "    path = [\n",
    "        term_dir / \"about_terms.csv\",\n",
    "        term_dir / \"color_terms.csv\",\n",
    "        term_dir / \"direction_terms.csv\",\n",
    "        term_dir / \"elevation_terms.csv\",\n",
    "        term_dir / \"geocoordinate_terms.csv\",\n",
    "        term_dir / \"habitat_terms.csv\",\n",
    "        term_dir / \"month_terms.csv\",\n",
    "        term_dir / \"name_terms.csv\",\n",
    "        term_dir / \"numeric_terms.csv\",\n",
    "        term_dir / \"us_location_terms.csv\",\n",
    "        flora_dir / \"missing_terms.csv\",\n",
    "        flora_dir / \"rank_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # We don't want all of the terms\n",
    "    ignore = set(\n",
    "        \"\"\"\n",
    "        numeric_units bad_habitat roman color_missing not_trs not_name\n",
    "    \"\"\".split()\n",
    "    )\n",
    "\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if t[\"label\"] not in ignore}\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "TOKENS = get_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466846f-26f4-4cef-b5ef-bf9fcb5bd3dc",
   "metadata": {},
   "source": [
    "## Alias the noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7110400-c771-41b7-8453-722d70dc5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOO_FEW = 1\n",
    "DOT_LIMIT = 5\n",
    "WORD_RE = re.compile(\n",
    "    r\"^ ( [\\p{L}\\p{M}]+ ('[st])? \\.? | [\\p{L}\\p{M}]+ [\\p{L}\\p{M}.]+ [\\p{L}\\p{M}]+ ) $\",\n",
    "    flags=re.X,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4189320c-6c72-4ff5-a2df-32bf190f8f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aliases_proc(limit, offset):\n",
    "    aliased = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        rows = list(\n",
    "            cxn.execute(\n",
    "                \"\"\"select * from normalized limit ? offset ?\"\"\",\n",
    "                (limit, offset),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for phrase, hits in rows:\n",
    "        try:\n",
    "            doc = nlp(phrase)\n",
    "        except ValueError:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        pattern = []\n",
    "        k = 0\n",
    "\n",
    "        for token in doc:\n",
    "            if hypernym := TOKENS.get(token.lower_):\n",
    "                pattern.append(f\"<{hypernym}>\")\n",
    "\n",
    "            elif token.pos_ in POS_ALIAS:\n",
    "                pattern.append(f\"<{token.pos_.lower()}>\")\n",
    "\n",
    "            elif token.is_punct or token.is_quote:\n",
    "                pattern.append(token.text)\n",
    "\n",
    "            else:\n",
    "                k += len(token)\n",
    "                pattern.append(token.lower_)\n",
    "\n",
    "        if k <= TOO_FEW:  # Not enuf non-token characters\n",
    "            continue\n",
    "\n",
    "        pattern = \" \".join(pattern)\n",
    "\n",
    "        aliased[pattern] += hits\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in aliased.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    csv_path = BELS_TEMP / f\"aliased_{offset}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574e795d-cbe6-445b-9d35-5cd79ffc6633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aliases():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from normalized\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    total = sum(1 for _ in range(0, count, limit))\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm(total=total) as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_aliases_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "# get_aliases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9c5381-32ea-43de-8a82-65c64d6812f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_aliases():\n",
    "    phrases = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(BELS_TEMP.glob(\"aliased_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                phrase = substitute(row[\"phrase\"], SUBS)\n",
    "                phrases[phrase] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(phrases)=}\")\n",
    "    print(f\"{sum(phrases.values())=}\")\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in phrases.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        df.to_sql(\"aliases\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "# save_aliases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b062c7-2558-4a66-9fbf-a82285b55b51",
   "metadata": {},
   "source": [
    "## Get locality vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9613737e-b870-452e-b682-41c5d6f52f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary():\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from aliases\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        rows = cxn.execute(\"\"\"select * from aliases\"\"\")\n",
    "\n",
    "        for phrase, hits in tqdm(rows, total=total):\n",
    "            for word in phrase.split():\n",
    "                word = word.strip()\n",
    "\n",
    "                if len(word) > DOT_LIMIT and word[-1] == \".\":\n",
    "                    word = word[:-1]\n",
    "\n",
    "                if WORD_RE.match(word):\n",
    "                    all_words[word] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"words\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "# get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dfe88-a865-497b-a111-de95b675134a",
   "metadata": {},
   "source": [
    "## Remove taxon names & other traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33a0629b-bf42-4ba4-a68f-d3a082e149c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_LIMIT = 1000\n",
    "\n",
    "\n",
    "def clean_words():\n",
    "    errors = 0\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    taxa_dir = Path(f_terms.__file__).parent\n",
    "    binomial_terms = taxa_dir / \"binomial_terms.zip\"\n",
    "    monomial_terms = taxa_dir / \"monomial_terms.zip\"\n",
    "\n",
    "    taxa = tu.read_terms([binomial_terms, monomial_terms])\n",
    "    taxa = {t[\"pattern\"] for t in taxa}  # for w in t[\"pattern\"].split()}\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from words\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        rows = cxn.execute(\"\"\"select * from words\"\"\")\n",
    "\n",
    "        for words, hits in tqdm(rows, total=total):\n",
    "            if words in taxa:\n",
    "                continue\n",
    "\n",
    "            for word in set(words.split()):\n",
    "                if word in taxa:\n",
    "                    continue\n",
    "\n",
    "                if len(word) <= 1:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    doc = nlp(word)\n",
    "                except ValueError:\n",
    "                    errors += 1\n",
    "                    continue\n",
    "\n",
    "                if doc[0].pos_ in (\"ADP\",) and hits < POS_LIMIT:\n",
    "                    continue\n",
    "\n",
    "                if doc[0].pos_ in (\"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"SCONJ\"):\n",
    "                    continue\n",
    "\n",
    "                all_words[word] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"cleaned\", cxn, index=False, if_exists=\"replace\")\n",
    "        return errors\n",
    "\n",
    "\n",
    "# clean_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7ca6043-03bf-48f6-b518-9fa93d3556ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_words(hits=1):\n",
    "    split = defaultdict(int)\n",
    "    \n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        rows = cxn.execute(\"\"\"select * from cleaned\"\"\")\n",
    "        for pattern, hits in rows:\n",
    "            words = pattern.split(\".\")\n",
    "            end = len(words) - 1\n",
    "            for i, word in enumerate(words):\n",
    "                if not word:\n",
    "                    continue\n",
    "                if i < end:\n",
    "                    split[f\"{word}.\"] += hits\n",
    "                else:\n",
    "                    split[word] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in split.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"cleaned\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "split_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acea716f-790a-46a4-a243-40fe9d3beb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_words(hits=1):\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        df = pd.read_sql(\n",
    "            \"select pattern from cleaned where hits > ? order by pattern\",\n",
    "            cxn,\n",
    "            params=[hits],\n",
    "        )\n",
    "        df.to_csv(LOCALITIES, index=False)\n",
    "\n",
    "\n",
    "write_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e48a0-5e5c-4d4b-91cf-190cd91013bb",
   "metadata": {},
   "source": [
    "# Parked code that may be useful later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d61d2-b8e9-4724-ab49-8f774b909a4e",
   "metadata": {},
   "source": [
    "## Get patterns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ca210ae-1b93-409c-88cd-645e44d1af40",
   "metadata": {
    "tags": []
   },
   "source": [
    "def get_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from aliases\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        words = {w[\"word\"] for w in cxn.execute(\"select pattern from words\")}\n",
    "        aliases = cxn.execute(\"select * from aliases\")\n",
    "\n",
    "        for phrase, hits in tqdm(aliases, total=total):\n",
    "            pattern = []\n",
    "\n",
    "            for word in phrase.split():\n",
    "                if word in words:\n",
    "                    if pattern and pattern[-1] == \"<loc>\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        pattern.append(\"<loc>\")\n",
    "\n",
    "                elif re.match(r\"^\\w+$\", word):\n",
    "                    pattern.append(\"<rt>\")\n",
    "\n",
    "                else:\n",
    "                    pattern.append(word)\n",
    "\n",
    "            pattern = \" \".join(pattern)\n",
    "            patterns[pattern] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in patterns.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"patterns\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "get_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2ce12-ae99-4f28-a98f-407e82392cac",
   "metadata": {},
   "source": [
    "## Fix pattern issues"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05fa1526-a8fc-4457-9b15-03aa00f22ddb",
   "metadata": {},
   "source": [
    "def fix_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    subs = [\n",
    "        (re.compile(r\" 's \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <number_word> \\s+ <imperial_dist> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <number_word> \\s+ <metric_dist> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <num> \\s+ <imperial_dist> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <num> \\s+ <metric_dist> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <num> \\s+ <imperial_length> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <num> \\s+ <metric_length> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ <( cconj | det | num | sconj )> \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" <( cconj | det | num | sconj )> $ \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" [;:,.'+=_/?-] $ \", flags=re.X), \" \"),\n",
    "        (re.compile(r\" ^ [;:,.'+=_/?-] \", flags=re.X), \" \"),\n",
    "    ]\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from patterns\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        recs = cxn.execute(\"select * from patterns\")\n",
    "\n",
    "        for pat, hits in tqdm(recs, total=total):\n",
    "            pat = substitute(pat, subs)\n",
    "            pat = \" \".join(pat.split())\n",
    "            patterns[pat] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in patterns.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"fixups\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "fix_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a1a46-17f1-43ea-9fd1-f881f7ab0846",
   "metadata": {},
   "source": [
    "## Merge patterns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edc4556d-7304-4fe1-afbb-ddd59134ac6f",
   "metadata": {},
   "source": [
    "def merge_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    subs = {f\"<{v}>\" for v in TOKENS.values()}\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from fixups\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        recs = cxn.execute(\"select * from fixups\")\n",
    "\n",
    "        for pat, hits in tqdm(recs, total=total):\n",
    "            new = [\"<loc>\" if t in subs else t for t in pat.split()]\n",
    "            new = \" \".join(new)\n",
    "            new = re.sub(r\"<loc> ( \\s+ <loc> )+ \", \"<loc>\", new, flags=re.X)\n",
    "            if new.find(\"�\") > -1:\n",
    "                continue\n",
    "            if new:\n",
    "                patterns[new] += hits\n",
    "\n",
    "        batch = [\n",
    "            {\"pattern\": k, \"hits\": v, \"n\": len(k.split())} for k, v in patterns.items()\n",
    "        ]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"merged\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "merge_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a509ba6-c4eb-4a66-91a3-ad6ab3c9af06",
   "metadata": {},
   "source": [
    "## Try building spacy patterns"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35f0ac0d-58cc-457d-9643-4ae5e2835e50",
   "metadata": {},
   "source": [
    "PUNCT = \"\"\"[¿?→><%#+!¦|@*\\。°.·,,:;/⁄‘’'``~“\"”„»«°=─—–\\-]\"\"\"\n",
    "TOKEN = \"\"\"<(loc|rt|num|cconj|det|sconj)>\"\"\"\n",
    "\n",
    "FINALS = [\n",
    "    re.compile(rf\"^ ( {PUNCT}+ \\s+ )? {TOKEN} \\s+ {PUNCT}+ $\", flags=re.X),\n",
    "    re.compile(\n",
    "        rf\"^ ( {PUNCT}+ \\s+ )? {TOKEN} ( \\s+ {PUNCT}* (\\s+ {PUNCT}*)* \\s* {TOKEN} )* $\",\n",
    "        flags=re.X,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "KILL = (PUNCT = \"\"\"[¿?→><%#+!¦|@*\\。°.·,,:;/⁄‘’'``~“\"”„»«°=─—–\\-]\"\"\"\n",
    "TOKEN = \"\"\"<(loc|rt|num|cconj|det|sconj)>\"\"\"\n",
    "\n",
    "FINALS = [\n",
    "    re.compile(rf\"^ ( {PUNCT}+ \\s+ )? {TOKEN} \\s+ {PUNCT}+ $\", flags=re.X),\n",
    "    re.compile(\n",
    "        rf\"^ ( {PUNCT}+ \\s+ )? {TOKEN} ( \\s+ {PUNCT}* (\\s+ {PUNCT}*)* \\s* {TOKEN} )* $\",\n",
    "        flags=re.X,\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "KILL = (\n",
    "    r\"[\\p{L}\\p{M}]+[\\p{P}\\p{N}©¡\\d√🏡]\"\n",
    "    r\"|[’`]\\w{1,2}\"\n",
    "    r\"|[©¡®🏡†|]\"\n",
    "    r\"|\\d[⁄/]|[⁄/]\\d\"\n",
    "    r\"|\\s\\w+\\s\"\n",
    "    rf\"|{PUNCT}\\s{PUNCT}\"\n",
    ")\n",
    "\n",
    "\n",
    "def spacy_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    subs = [\n",
    "        (re.compile(rf\"(\\s{PUNCT})+$\"), \"\"),\n",
    "        (re.compile(r\"\\d+[\\d.]*\"), \" \"),\n",
    "        # (re.compile(r\"[|🏡†]\"), \" \"),\n",
    "    ]\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from merged where hits > 1\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        recs = cxn.execute(\"select * from merged where hits > 1 order by hits desc, n\")\n",
    "\n",
    "        miss = 0\n",
    "        skip = 0\n",
    "        for pat, hits, n in tqdm(recs, total=total):\n",
    "            pat = substitute(pat, subs)\n",
    "            pat = \" \".join(pat.split())\n",
    "\n",
    "            if re.search(KILL, pat):\n",
    "                skip += 1\n",
    "                continue\n",
    "\n",
    "            for final in FINALS:\n",
    "                if final.match(pat):\n",
    "                    break\n",
    "            else:\n",
    "                print(pat)\n",
    "                miss += 1\n",
    "                if miss > 9:\n",
    "                    return\n",
    "    print(skip)\n",
    "    r\"[\\p{L}\\p{M}]+[\\p{P}\\p{N}©¡\\d√🏡]\"\n",
    "    r\"|[’`]\\w{1,2}\"\n",
    "    r\"|[©¡®🏡†|]\"\n",
    "    r\"|\\d[⁄/]|[⁄/]\\d\"\n",
    "    r\"|\\s\\w+\\s\"\n",
    "    rf\"|{PUNCT}\\s{PUNCT}\"\n",
    ")\n",
    "\n",
    "\n",
    "def spacy_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    subs = [\n",
    "        (re.compile(rf\"(\\s{PUNCT})+$\"), \"\"),\n",
    "        (re.compile(r\"\\d+[\\d.]*\"), \" \"),\n",
    "        # (re.compile(r\"[|🏡†]\"), \" \"),\n",
    "    ]\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from merged where hits > 1\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        recs = cxn.execute(\"select * from merged where hits > 1 order by hits desc, n\")\n",
    "\n",
    "        miss = 0\n",
    "        skip = 0\n",
    "        for pat, hits, n in tqdm(recs, total=total):\n",
    "            pat = substitute(pat, subs)\n",
    "            pat = \" \".join(pat.split())\n",
    "\n",
    "            if re.search(KILL, pat):\n",
    "                skip += 1\n",
    "                continue\n",
    "\n",
    "            for final in FINALS:\n",
    "                if final.match(pat):\n",
    "                    break\n",
    "            else:\n",
    "                print(pat)\n",
    "                miss += 1\n",
    "                if miss > 9:\n",
    "                    return\n",
    "    print(skip)\n",
    "\n",
    "\n",
    "spacy_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dcb87-c125-476f-a102-ce00bc524e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
