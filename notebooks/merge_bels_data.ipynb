{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the BELS gazetteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "BELS has a different goal than Digi-Leap BELS is trying to pinpoint latitudes and longitudes from strings and Digi-Leap is parsing strings that may then be fed into BELS. I'm taking the data from BELS and trying to reduce it to a smaller number of patterns that can be fed into spaCy rule parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plants'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mregex\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplants\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraits\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m terms \u001b[38;5;28;01mas\u001b[39;00m p_terms\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraiter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpylib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m term_util \u001b[38;5;28;01mas\u001b[39;00m tu\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plants'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from flora.pylib.traits import terms as p_terms\n",
    "from tqdm.notebook import tqdm\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c4f7ce-becf-469e-85a6-5fd7475cf743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47d021-c349-473e-93ff-15a98ad32130",
   "metadata": {},
   "source": [
    "## BELS localities given to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bd20a5-92c9-463d-8b9a-285f8bb2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELS = DATA_DIR / \"bels\"\n",
    "BELS_DB = BELS / \"localities.sqlite\"\n",
    "BELS_ORI = BELS / \"original\"\n",
    "BELS_TEMP = BELS / \"temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dba32d-0660-4804-a559-dd8c43abf92d",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b191f7b4-31cb-4cef-bd68-aaa77fed4588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    paths = sorted(BELS_ORI.glob(\"*.csv.gz\"))\n",
    "\n",
    "    if_exists = \"replace\"\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        for path in tqdm(paths):\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            locs = df[\"v_locality\"]\n",
    "            locs.to_sql(\"raw\", cxn, index=False, if_exists=if_exists)\n",
    "\n",
    "            if_exists = \"append\"\n",
    "\n",
    "\n",
    "# get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "\n",
    "too_short = 3\n",
    "\n",
    "punct = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "subs = [\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {punct} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading punct\n",
    "    (re.compile(rf\"^( \\s* {punct} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing punct\n",
    "    (re.compile(rf\"( \\s* {punct} \\s* )+ $\", flags=re.X), \" \"),\n",
    "    # Handle contractions\n",
    "    (re.compile(r\" \\s ( '[st] ) \", flags=re.X), r\"\\1\"),\n",
    "    # Handle abbreviations\n",
    "    (re.compile(r\" ([\\p{L}\\p{M}]{1,4}) \\s ( \\. ) \", flags=re.X), r\"\\1\\2\"),\n",
    "    # Remove back slashes\n",
    "    (re.compile(r\"\\\\\", flags=re.X), \"\"),\n",
    "]\n",
    "\n",
    "subs2 = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "]\n",
    "subs2 += subs\n",
    "\n",
    "# Character classes\n",
    "controls = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "symbols = \" Sc \".split()  # Currency symbols\n",
    "separators = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "remove = controls + symbols + separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, repl in subs:\n",
    "            loc = regexp.sub(repl, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in remove else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        loc = substitute(loc, subs)\n",
    "\n",
    "        # Some more aggressive replacements\n",
    "        loc = substitute(loc, subs2)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= too_short:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from raw\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        for loc in tqdm(cxn.execute(\"\"\"select * from raw\"\"\"), total=total):\n",
    "            loc = loc[\"v_locality\"]\n",
    "\n",
    "            norm = normalize_location(loc)\n",
    "            if norm.loc:\n",
    "                normals[norm.loc] += norm.add\n",
    "            errors += norm.error\n",
    "\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff63a5-5d25-4681-bf7e-24b16f901829",
   "metadata": {},
   "source": [
    "## Alias localities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9176fac-d1d2-4d36-8348-39dd21a9b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get words that get replaced in the BELS noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b45189-dba6-4f87-a8eb-52d879bd55fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The are sets of common terms and patterns stored in CSV files that are used to categorize word or phrase types like colors or dates. We're going to use them to replace terms in the BELS noun phrases with hypernyms. For instance:\n",
    "\n",
    "- Replace `12 North Main Street` with `<num> <dir> main street`\n",
    "\n",
    "The hope is to cut down on the total number of patterns stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a72d3b3e-713d-4778-a4c4-853d7cfd1f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    TERM_DIR = Path(t_terms.__file__).parent\n",
    "    tokens = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get units\n",
    "    path = [\n",
    "        TERM_DIR / \"unit_distance_terms.csv\",\n",
    "        TERM_DIR / \"unit_length_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # Skip anything smaller than a foot\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if float(t[\"factor_cm\"]) > 30.0}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get other tokens\n",
    "    path = [\n",
    "        TERM_DIR / \"about_terms.csv\",\n",
    "        TERM_DIR / \"color_terms.csv\",\n",
    "        TERM_DIR / \"direction_terms.csv\",\n",
    "        TERM_DIR / \"elevation_terms.csv\",\n",
    "        TERM_DIR / \"geocoordinate_terms.csv\",\n",
    "        TERM_DIR / \"habitat_terms.csv\",\n",
    "        TERM_DIR / \"month_terms.csv\",\n",
    "        TERM_DIR / \"name_terms.csv\",\n",
    "        TERM_DIR / \"numeric_terms.csv\",\n",
    "        TERM_DIR / \"us_location_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # We don't want all of the terms\n",
    "    ignore = set(\"\"\"\n",
    "        numeric_units bad_habitat roman color_missing not_trs not_name\n",
    "    \"\"\".split())\n",
    "\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if t[\"label\"] not in ignore}\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "TOKENS = get_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466846f-26f4-4cef-b5ef-bf9fcb5bd3dc",
   "metadata": {},
   "source": [
    "## Alias the noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7110400-c771-41b7-8453-722d70dc5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "too_short = 1\n",
    "dot_limit = 5\n",
    "word_re = re.compile(r\"^ [\\p{L}\\p{M}]+ ('[st])? \\.? $\", flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4189320c-6c72-4ff5-a2df-32bf190f8f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aliases_proc(limit, offset):\n",
    "    aliased = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        rows = list(\n",
    "            cxn.execute(\n",
    "                \"\"\"select * from normalized limit ? offset ?\"\"\",\n",
    "                (limit, offset),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for phrase, hits in rows:\n",
    "        try:\n",
    "            doc = nlp(phrase)\n",
    "        except ValueError:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        pattern = []\n",
    "        k = 0\n",
    "\n",
    "        for token in doc:\n",
    "            if hypernym := TOKENS.get(token.lower_):\n",
    "                pattern.append(f\"<{hypernym}>\")\n",
    "\n",
    "            elif token.like_num:\n",
    "                pattern.append(\"<num>\")\n",
    "\n",
    "            elif token.is_punct or token.is_quote:\n",
    "                pattern.append(token.text)\n",
    "\n",
    "            else:\n",
    "                k += len(token)\n",
    "                pattern.append(token.lower_)\n",
    "\n",
    "        if k <= too_short:  # Not enuf non-token characters\n",
    "            continue\n",
    "\n",
    "        pattern = \" \".join(pattern)\n",
    "\n",
    "        aliased[pattern] += hits\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in aliased.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    csv_path = BELS_TEMP / f\"aliased_{offset}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574e795d-cbe6-445b-9d35-5cd79ffc6633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b49eeed5ef4966bcc715f90c0ed508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_aliases():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from normalized\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    total = sum(1 for _ in range(0, count, limit))\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm(total=total) as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_aliases_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "get_aliases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe9c5381-32ea-43de-8a82-65c64d6812f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2735dd76561540b5bcf420f7b541f432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(phrases)=32132962\n",
      "sum(phrases.values())=38239537\n"
     ]
    }
   ],
   "source": [
    "def save_aliases():\n",
    "    phrases = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(BELS_TEMP.glob(\"aliased_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                phrases[row[\"phrase\"]] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(phrases)=}\")\n",
    "    print(f\"{sum(phrases.values())=}\")\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in phrases.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        df.to_sql(\"aliases\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "save_aliases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b062c7-2558-4a66-9fbf-a82285b55b51",
   "metadata": {},
   "source": [
    "## Get locality vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9613737e-b870-452e-b682-41c5d6f52f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39ca00da549436b895f3b7b54debbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32132962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_vocabulary():\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from aliases\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        rows = cxn.execute(\"\"\"select * from aliases\"\"\")\n",
    "\n",
    "        for phrase, hits in tqdm(rows, total=total):\n",
    "            for word in phrase.split():\n",
    "                word = word.strip()\n",
    "\n",
    "                if len(word) > dot_limit and word[-1] == \".\":\n",
    "                    word = word[:-1]\n",
    "\n",
    "                if word_re.match(word):\n",
    "                    all_words[word] += hits\n",
    "\n",
    "        batch = [{\"word\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"words\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acea716f-790a-46a4-a243-40fe9d3beb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_words(hits=1):\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        df = pd.read_sql(\n",
    "            \"select word from words where hits > ? order by word\", cxn, params=[hits]\n",
    "        )\n",
    "        df.to_csv(BELS / \"localities.csv\", index=False)\n",
    "\n",
    "\n",
    "write_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d61d2-b8e9-4724-ab49-8f774b909a4e",
   "metadata": {},
   "source": [
    "## Get patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79cea6c1-7d32-444c-a9a8-f34041cd584f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c038c6df44d84197b57aa1f99cabe671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32132962 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_patterns():\n",
    "    patterns = defaultdict(int)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        cur = cxn.execute(\"\"\"select count(*) from aliases\"\"\")\n",
    "        total = cur.fetchone()[0]\n",
    "\n",
    "        words = {w[\"word\"] for w in cxn.execute(\"select word from words\")}\n",
    "        aliases = cxn.execute(\"select * from aliases\")\n",
    "\n",
    "        for phrase, hits in tqdm(aliases, total=total):\n",
    "            pattern = []\n",
    "\n",
    "            for word in phrase.split():\n",
    "                if word in words:\n",
    "                    if pattern and pattern[-1] == \"<loc>\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        pattern.append(\"<loc>\")\n",
    "\n",
    "                elif re.match(r\"^\\w+$\", word):\n",
    "                    pattern.append(\"<rt>\")\n",
    "\n",
    "                else:\n",
    "                    pattern.append(word)\n",
    "\n",
    "            pattern = \" \".join(pattern)\n",
    "            patterns[pattern] += hits\n",
    "\n",
    "        batch = [{\"pattern\": k, \"hits\": v} for k, v in patterns.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"patterns\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "get_patterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad0b35-7ab1-46e0-a8cf-3c54dd9fa7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
