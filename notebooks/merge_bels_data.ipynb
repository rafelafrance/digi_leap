{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the BELS gazetteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "BELS has a different goal than Digi-Leap BELS is trying to pinpoint latitudes and longitudes from strings and Digi-Leap is parsing strings that may then be fed into BELS. I'm taking the data from BELS and trying to reduce it to a smaller number of patterns that can be fed into spaCy rule parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from traiter.pylib import const\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c4f7ce-becf-469e-85a6-5fd7475cf743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47d021-c349-473e-93ff-15a98ad32130",
   "metadata": {},
   "source": [
    "## BELS localities given to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bd20a5-92c9-463d-8b9a-285f8bb2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELS = DATA_DIR / \"bels\"\n",
    "BELS_DB = BELS / \"localities.sqlite\"\n",
    "BELS_ORI = BELS / \"original\"\n",
    "BELS_TEMP = BELS / \"temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dba32d-0660-4804-a559-dd8c43abf92d",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b191f7b4-31cb-4cef-bd68-aaa77fed4588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    paths = sorted(BELS_ORI.glob(\"*.csv.gz\"))\n",
    "\n",
    "    if_exists = \"replace\"\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        for path in tqdm(paths):\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            locs = df[\"v_locality\"]\n",
    "            locs.to_sql(\"raw\", cxn, index=False, if_exists=if_exists)\n",
    "\n",
    "            if_exists = \"append\"\n",
    "\n",
    "\n",
    "# get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "\n",
    "too_short = 3\n",
    "\n",
    "punct = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "subs = [\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {punct} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading punct\n",
    "    (re.compile(rf\"^( \\s* {punct} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing punct\n",
    "    (re.compile(rf\"( \\s* {punct} \\s* )+ $\", flags=re.X), \" \"),\n",
    "]\n",
    "\n",
    "subs2 = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "]\n",
    "subs2 += subs\n",
    "\n",
    "# Character classes\n",
    "controls = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "symbols = \" Sc \".split()  # Currency symbols\n",
    "separators = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "remove = controls + symbols + separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, replace in subs:\n",
    "            loc = regexp.sub(replace, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in remove else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        # loc = replace(loc, subs)\n",
    "\n",
    "        # Some more aggressive replacements\n",
    "        loc = replace(loc, subs2)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= too_short:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177e307a-2062-4872-af69-89618152f09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize_location('\"aasvogelberg, an steinigen oertern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        for loc in tqdm(cxn.execute(\"\"\"select * from raw\"\"\")):\n",
    "            loc = loc[\"v_locality\"]\n",
    "\n",
    "            norm = normalize_location(loc)\n",
    "            if norm.loc:\n",
    "                normals[norm.loc] += norm.add\n",
    "            errors += norm.error\n",
    "\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff63a5-5d25-4681-bf7e-24b16f901829",
   "metadata": {},
   "source": [
    "## Alias localities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9176fac-d1d2-4d36-8348-39dd21a9b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get words that get replaced in the BELS noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b45189-dba6-4f87-a8eb-52d879bd55fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The are sets of common terms and patterns stored in CSV files that are used to categorize word or phrase types like colors or dates. We're going to use them to replace terms in the BELS noun phrases with hypernyms. For instance:\n",
    "\n",
    "- Replace `12 North Main Street` with `<num> <dir> main street`\n",
    "\n",
    "The hope is to cut down on the total number of patterns stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72d3b3e-713d-4778-a4c4-853d7cfd1f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    TERM_DIR = Path(t_terms.__file__).parent\n",
    "    tokens = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get units\n",
    "    path = [\n",
    "        TERM_DIR / \"unit_distance_terms.csv\",\n",
    "        TERM_DIR / \"unit_length_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # Skip anything smaller than a foot\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if float(t[\"factor_cm\"]) > 30.0}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get other tokens\n",
    "    path = [\n",
    "        TERM_DIR / \"color_terms.csv\",\n",
    "        TERM_DIR / \"direction_terms.csv\",\n",
    "        TERM_DIR / \"habitat_terms.csv\",\n",
    "        TERM_DIR / \"numeric_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # We don't want all of the terms\n",
    "    ignore = \"\"\" numeric_units bad_habitat roman color_missing \"\"\".split()\n",
    "\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if t[\"label\"] not in ignore}\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "TOKENS = get_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466846f-26f4-4cef-b5ef-bf9fcb5bd3dc",
   "metadata": {},
   "source": [
    "## Alias the noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4189320c-6c72-4ff5-a2df-32bf190f8f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "too_short = 1\n",
    "\n",
    "\n",
    "def get_aliases_proc(limit, offset):\n",
    "    aliased = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        rows = list(\n",
    "            cxn.execute(\n",
    "                \"\"\"select * from phrases limit ? offset ?\"\"\",\n",
    "                (limit, offset),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for row in rows:\n",
    "        phrase, hits = row\n",
    "\n",
    "        try:\n",
    "            doc = nlp(phrase)\n",
    "        except ValueError:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        pattern = []\n",
    "        k = 0\n",
    "\n",
    "        for token in doc:\n",
    "            if hypernym := TOKENS.get(token.lower_):\n",
    "                pattern.append(f\"<{hypernym}>\")\n",
    "\n",
    "            elif token.like_num:\n",
    "                pattern.append(\"<num>\")\n",
    "\n",
    "            elif token.is_punct or token.is_quote:\n",
    "                pattern.append(token.text)\n",
    "\n",
    "            else:\n",
    "                k += len(token)\n",
    "                pattern.append(token.lower_)\n",
    "\n",
    "        if k <= too_short:  # Not enuf non-token characters\n",
    "            continue\n",
    "\n",
    "        pattern = \" \".join(pattern)\n",
    "        aliased[pattern] += hits\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in aliased.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    csv_path = BELS_TEMP / f\"aliased_{offset}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574e795d-cbe6-445b-9d35-5cd79ffc6633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aliases():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from phrases\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    total = sum(1 for _ in range(0, count, limit))\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm(total=total) as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_aliases_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "# get_aliases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9c5381-32ea-43de-8a82-65c64d6812f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_aliases():\n",
    "    phrases = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(BELS_TEMP.glob(\"aliased_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                phrases[row[\"phrase\"]] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(phrases)=}\")\n",
    "    print(f\"{sum(phrases.values())=}\")\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in phrases.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        df.to_sql(\"aliases\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "# save_aliases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b062c7-2558-4a66-9fbf-a82285b55b51",
   "metadata": {},
   "source": [
    "## Get locality vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb7594c-d083-4229-b68f-20de02d1cae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c123f4c30141d6ab2db1924fb55bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_re = re.compile(r\"^ \\p{L}+ $\", flags=re.X)\n",
    "\n",
    "\n",
    "def get_vocabulary():\n",
    "    all_words = defaultdict(int)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        rows = cxn.execute(\"\"\"select * from aliases\"\"\")\n",
    "\n",
    "        for row in tqdm(rows):\n",
    "            phrase, hits = row\n",
    "            for word in phrase.split():\n",
    "                if word_re.match(word):\n",
    "                    all_words[word] += hits\n",
    "\n",
    "        batch = [{\"word\": k, \"hits\": v} for k, v in all_words.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "\n",
    "        df.to_sql(\"words\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "# get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acea716f-790a-46a4-a243-40fe9d3beb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_words(hits=1):\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        df = pd.read_sql(\n",
    "            \"select word from words where hits > ? order by word\", cxn, params=[hits]\n",
    "        )\n",
    "        df.to_csv(BELS / \"localities.csv\", index=False)\n",
    "\n",
    "\n",
    "write_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad0b35-7ab1-46e0-a8cf-3c54dd9fa7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
