{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c811ad39-5d9c-4a98-a85c-876866cee1b1",
   "metadata": {},
   "source": [
    "# Build locality terms from the BELS gazetteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200a9f6b-7934-492f-ba7a-533c16bbafec",
   "metadata": {},
   "source": [
    "BELS has a different goal than Digi-Leap BELS is trying to pinpoint latitudes and longitudes from strings and Digi-Leap is parsing strings that may then be fed into BELS. I'm taking the data from BELS and trying to reduce it to a smaller number of patterns that can be fed into spaCy rule parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f7b613-0718-4d47-aa68-98dc815034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import html\n",
    "import sqlite3\n",
    "import unicodedata as uni\n",
    "from collections import defaultdict, namedtuple\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from traiter.pylib import const\n",
    "from traiter.pylib import term_util as tu\n",
    "from traiter.pylib.pipes import extensions, tokenizer\n",
    "from traiter.pylib.traits import terms as t_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c4f7ce-becf-469e-85a6-5fd7475cf743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"..\") / \"data\"\n",
    "\n",
    "PROCESSES = 16  # Number of parallel processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33a35c6-459e-43c7-b583-8028421b28d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Location = namedtuple(\"Location\", \"loc add error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec47d021-c349-473e-93ff-15a98ad32130",
   "metadata": {},
   "source": [
    "## BELS localities given to me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bd20a5-92c9-463d-8b9a-285f8bb2a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "BELS = DATA_DIR / \"bels\"\n",
    "BELS_DB = BELS / \"localities.sqlite\"\n",
    "BELS_ORI = BELS / \"original\"\n",
    "BELS_TEMP = BELS / \"temp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3391-3e42-42fc-b8f4-240bfb14283e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978c3136-9415-487d-9626-ee860f920b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions.add_extensions()\n",
    "nlp = spacy.load(\"en_core_web_md\", exclude=[\"ner\"])\n",
    "tokenizer.setup_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dba32d-0660-4804-a559-dd8c43abf92d",
   "metadata": {},
   "source": [
    "## Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b191f7b4-31cb-4cef-bd68-aaa77fed4588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    paths = sorted(BELS_ORI.glob(\"*.csv.gz\"))\n",
    "\n",
    "    if_exists = \"replace\"\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        for path in tqdm(paths):\n",
    "            df = pd.read_csv(path)\n",
    "\n",
    "            locs = df[\"v_locality\"]\n",
    "            locs.to_sql(\"raw\", cxn, index=False, if_exists=if_exists)\n",
    "\n",
    "            if_exists = \"append\"\n",
    "\n",
    "\n",
    "# get_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18caa1a9-d45e-4c72-b306-c3f1986870e5",
   "metadata": {},
   "source": [
    "## Basic locality normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf2e57-5dc4-4dd1-99db-ddd50c6263d6",
   "metadata": {},
   "source": [
    "See [here](https://en.wikipedia.org/wiki/Unicode_character_property) for a description of the character class abbreviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad21444-32f4-4967-b2ea-bd4216818625",
   "metadata": {},
   "source": [
    "The raw localities are rough, perform some simple steps to improve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ee5c24-5faf-44e3-904b-d1e80a468aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = 0\n",
    "\n",
    "too_short = 3\n",
    "\n",
    "punct = \"\"\"[&%$#!*,/;.:?'\"_-]+\"\"\"\n",
    "\n",
    "subs = [\n",
    "    # Like (...)\n",
    "    (re.compile(rf\"\\(+ {punct} \\)+\", flags=re.X), \" \"),\n",
    "    # Like (9)\n",
    "    (re.compile(r\"\\(+ \\s* \\d* \\s* \\)+ \", flags=re.X), \" \"),\n",
    "    # Lat/long\n",
    "    (re.compile(r\"\\(? [\\d.-]+ [\\s,]+ [\\d.-]+ \\)?\", flags=re.X), \" \"),\n",
    "    # CSV delimiters? The question marks are odd, I admit\n",
    "    (re.compile(r\"[.,?]{2,}\"), \" \"),\n",
    "    # Enclosing quotes\n",
    "    (re.compile(r\"\"\"^ [({\\['\"/] \\s* (.+) \\s* [\\]})'\"/] $\"\"\", flags=re.X), r\"\\1\"),\n",
    "    # Leading punct\n",
    "    (re.compile(rf\"^( \\s* {punct} \\s* )+\", flags=re.X), \" \"),\n",
    "    # Trailing punct\n",
    "    (re.compile(rf\"( \\s* {punct} \\s* )+ $\", flags=re.X), \" \"),\n",
    "]\n",
    "\n",
    "subs2 = [\n",
    "    # Agressively remove brackets\n",
    "    (re.compile(r\" [()\\[\\]\\{\\}]+ \", flags=re.X), \" \"),\n",
    "]\n",
    "subs2 += subs\n",
    "\n",
    "# Character classes\n",
    "controls = \" Cc Cf Cs Co Cn \".split()  # All control characters\n",
    "symbols = \" Sc \".split()  # Currency symbols\n",
    "separators = \" Zl Zp \".split()  # Line & paragraph separators\n",
    "remove = controls + symbols + separators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe956a2-0d25-49e8-802a-c4701f9a406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(loc, subs):\n",
    "    prev = \"\"\n",
    "    while prev != loc:\n",
    "        prev = loc\n",
    "        for regexp, replace in subs:\n",
    "            loc = regexp.sub(replace, loc)\n",
    "            loc = loc.strip()\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2bd6976-f1a7-45a0-a9f7-fd28ffb35c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    try:\n",
    "        # Replace HTML entities\n",
    "        loc = html.unescape(loc)\n",
    "\n",
    "        # Lower case the string\n",
    "        loc = loc.lower()\n",
    "\n",
    "        # Remove control characters & some punct\n",
    "        loc = [\" \" if uni.category(c) in remove else c for c in loc]\n",
    "        loc = \"\".join(loc)\n",
    "\n",
    "        # Normalize chars to ASCII\n",
    "        loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "        # Do some replacements\n",
    "        # loc = replace(loc, subs)\n",
    "\n",
    "        # Some more aggressive replacements\n",
    "        loc = replace(loc, subs2)\n",
    "\n",
    "        # Normalize spaces\n",
    "        loc = \" \".join(loc.split())\n",
    "\n",
    "        # Too short\n",
    "        if len(loc) <= too_short:\n",
    "            raise ValueError\n",
    "\n",
    "        # Add it\n",
    "        return Location(loc=loc, add=1, error=0)\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        return Location(loc=\"\", add=0, error=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177e307a-2062-4872-af69-89618152f09e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize_location('\"aasvogelberg, an steinigen oertern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b354094b-1658-4e42-ad91-44d7160b974d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize():\n",
    "    normals = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "\n",
    "        for loc in tqdm(cxn.execute(\"\"\"select * from raw\"\"\")):\n",
    "            loc = loc[\"v_locality\"]\n",
    "\n",
    "            norm = normalize_location(loc)\n",
    "            if norm.loc:\n",
    "                normals[norm.loc] += norm.add\n",
    "            errors += norm.error\n",
    "\n",
    "        batch = [{\"locality\": k, \"hits\": v} for k, v in normals.items()]\n",
    "        df = pd.DataFrame(batch)\n",
    "        df.to_sql(\"normalized\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "    return errors\n",
    "\n",
    "\n",
    "# normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff63a5-5d25-4681-bf7e-24b16f901829",
   "metadata": {},
   "source": [
    "## Alias localities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9176fac-d1d2-4d36-8348-39dd21a9b102",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get words that get replaced in the BELS noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b45189-dba6-4f87-a8eb-52d879bd55fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "The are sets of common terms and patterns stored in CSV files that are used to categorize word or phrase types like colors or dates. We're going to use them to replace terms in the BELS noun phrases with hypernyms. For instance:\n",
    "\n",
    "- Replace `12 North Main Street` with `<num> <dir> main street`\n",
    "\n",
    "The hope is to cut down on the total number of patterns stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72d3b3e-713d-4778-a4c4-853d7cfd1f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tokens():\n",
    "    TERM_DIR = Path(t_terms.__file__).parent\n",
    "    tokens = {}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get units\n",
    "    path = [\n",
    "        TERM_DIR / \"unit_distance_terms.csv\",\n",
    "        TERM_DIR / \"unit_length_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # Skip anything smaller than a foot\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if float(t[\"factor_cm\"]) > 30.0}\n",
    "\n",
    "    # ---------------------------\n",
    "    # Get other tokens\n",
    "    path = [\n",
    "        TERM_DIR / \"color_terms.csv\",\n",
    "        TERM_DIR / \"direction_terms.csv\",\n",
    "        TERM_DIR / \"habitat_terms.csv\",\n",
    "        TERM_DIR / \"numeric_terms.csv\",\n",
    "    ]\n",
    "\n",
    "    terms = tu.read_terms(path)\n",
    "\n",
    "    # We don't want all of the terms\n",
    "    ignore = \"\"\" numeric_units bad_habitat roman color_missing \"\"\".split()\n",
    "\n",
    "    tokens |= {t[\"pattern\"]: t[\"label\"] for t in terms if t[\"label\"] not in ignore}\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "TOKENS = get_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466846f-26f4-4cef-b5ef-bf9fcb5bd3dc",
   "metadata": {},
   "source": [
    "## Gather noun phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73272f78-c9c3-4aad-9042-afb85003037a",
   "metadata": {},
   "source": [
    "Use spaCy's language model to identify noun phrases and then replace words in them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4189320c-6c72-4ff5-a2df-32bf190f8f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_phrases_proc(limit, offset):\n",
    "    phrases = defaultdict(int)\n",
    "    errors = 0\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cxn.row_factory = sqlite3.Row\n",
    "        rows = list(\n",
    "            cxn.execute(\n",
    "                \"\"\"select * from normalized limit ? offset ?\"\"\",\n",
    "                (limit, offset),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for row in rows:\n",
    "        loc, hits = row\n",
    "\n",
    "        try:\n",
    "            doc = nlp(loc)\n",
    "        except ValueError:\n",
    "            errors += 1\n",
    "            continue\n",
    "\n",
    "        for chunk in doc.noun_chunks:\n",
    "            norm = normalize_location(chunk.text)\n",
    "            if norm.loc:\n",
    "                phrases[norm.loc] += norm.add\n",
    "            errors += norm.error\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in phrases.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    csv_path = BELS_TEMP / f\"phrases_{offset}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "574e795d-cbe6-445b-9d35-5cd79ffc6633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5588e625dd984beea508031d1f0ed208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "11096238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_phrases():\n",
    "    processes = 12\n",
    "    limit = 1_000_000\n",
    "\n",
    "    results = []\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        cur = cxn.execute(\"\"\"select count(*) from normalized\"\"\")\n",
    "        count = cur.fetchone()[0]\n",
    "\n",
    "    with Pool(processes=processes) as pool, tqdm() as bar:\n",
    "        for offset in range(0, count, limit):\n",
    "            results.append(\n",
    "                pool.apply_async(\n",
    "                    get_phrases_proc,\n",
    "                    args=(limit, offset),\n",
    "                    callback=lambda _: bar.update(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "get_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe9c5381-32ea-43de-8a82-65c64d6812f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f911297995c1455799266dfe47aab177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(phrases)=20370305\n",
      "sum(phrases.values())=87833445\n"
     ]
    }
   ],
   "source": [
    "def save_phrases():\n",
    "    phrases = defaultdict(int)\n",
    "\n",
    "    for path in tqdm(sorted(BELS_TEMP.glob(\"phrases_*.csv\"))):\n",
    "        with open(path) as csv_file:\n",
    "            reader = csv.DictReader(csv_file)\n",
    "\n",
    "            for row in reader:\n",
    "                phrases[row[\"phrase\"]] += int(row[\"hits\"])\n",
    "\n",
    "    print(f\"{len(phrases)=}\")\n",
    "    print(f\"{sum(phrases.values())=}\")\n",
    "\n",
    "    batch = [{\"phrase\": k, \"hits\": v} for k, v in phrases.items()]\n",
    "    df = pd.DataFrame(batch)\n",
    "\n",
    "    with sqlite3.connect(BELS_DB) as cxn:\n",
    "        df.to_sql(\"phrases\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "save_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b7618a5-748a-4018-99cf-2078b000225a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def alias_phrases_proc(path):\n",
    "#     aliasd = defaultdict(int)\n",
    "#     errors = 0\n",
    "\n",
    "#     df = pd.read_csv(path)\n",
    "\n",
    "#     for _, row in df.iterrows():\n",
    "#         try:\n",
    "#             phrase = str(row[\"phrase\"])\n",
    "\n",
    "#             phrase = [c for c in phrase if uni.category(c) not in CHAR_CLASS]\n",
    "\n",
    "#             phrase = \"\".join(phrase)\n",
    "#             phrase = \" \".join(phrase.split())\n",
    "\n",
    "#             doc = nlp(phrase)\n",
    "#         except ValueError:\n",
    "#             errors += 1\n",
    "#             continue\n",
    "\n",
    "#         pattern = []\n",
    "#         k = 0\n",
    "\n",
    "#         for token in doc:\n",
    "#             if hypernym := TOKENS.get(token.lower_):\n",
    "#                 pattern.append(f\"<{hypernym}>\")\n",
    "\n",
    "#             elif token.like_num:\n",
    "#                 pattern.append(\"<num>\")\n",
    "\n",
    "#             elif token.is_punct or token.is_quote:\n",
    "#                 pattern.append(token.text)\n",
    "\n",
    "#             else:\n",
    "#                 k += len(token)\n",
    "#                 pattern.append(token.lower_)\n",
    "\n",
    "#         if k <= 1:  # Not enuf non-token characters\n",
    "#             continue\n",
    "\n",
    "#         pattern = \" \".join(pattern)\n",
    "#         aliasd[pattern] += row[\"hits\"]\n",
    "\n",
    "#     batch = [{\"phrase\": k, \"hits\": v} for k, v in aliasd.items()]\n",
    "#     df = pd.DataFrame(batch)\n",
    "\n",
    "#     csv_path = BELS_ALIASED / f\"{path.stem}.csv\"\n",
    "#     df.to_csv(csv_path, index=False)\n",
    "\n",
    "#     return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def60756-1abe-4da7-9111-4beb18d1872b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def alias_phrases():\n",
    "#     paths = sorted(BELS_PHRASES.glob(\"*.csv\"))\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     with Pool(processes=PROCESSES) as pool, tqdm(total=len(paths)) as bar:\n",
    "#         for path in paths:\n",
    "#             results.append(\n",
    "#                 pool.apply_async(\n",
    "#                     alias_phrases_proc,\n",
    "#                     args=(path,),\n",
    "#                     callback=lambda _: bar.update(),\n",
    "#                 )\n",
    "#             )\n",
    "#         return sum(r.get() for r in results)\n",
    "\n",
    "\n",
    "# # alias_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a9c7b74-50e1-4fdc-91dd-396f3c067fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_aliased():\n",
    "#     aliased = defaultdict(int)\n",
    "\n",
    "#     for path in tqdm(sorted(BELS_ALIASED.glob(\"*.csv\"))):\n",
    "#         with open(path) as csv_file:\n",
    "#             reader = csv.DictReader(csv_file)\n",
    "\n",
    "#             for row in reader:\n",
    "#                 aliased[row[\"phrase\"]] += int(row[\"hits\"])\n",
    "\n",
    "#     print(f\"{len(aliased)=}\")\n",
    "#     print(f\"{sum(aliased.values())=}\")\n",
    "\n",
    "#     batch = [{\"phrase\": k, \"hits\": v} for k, v in aliased.items()]\n",
    "#     df = pd.DataFrame(batch)\n",
    "\n",
    "#     with sqlite3.connect(BELS_DB) as cxn:\n",
    "#         df.to_sql(\"aliased\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "\n",
    "# save_aliased()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a19cb-f5f7-4611-ac04-58de0bce8676",
   "metadata": {},
   "source": [
    "## Look at facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c86790-d190-4d4f-af8b-82641cb8df8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_fingerprints():\n",
    "#     facets = defaultdict(int)\n",
    "#     errors = 0\n",
    "#     remove = CONTROLS + PUNCTS + SYMBOLS + SPACES + DIGITS\n",
    "\n",
    "#     with sqlite3.connect(BELS_DB) as cxn:\n",
    "#         cxn.row_factory = sqlite3.Row\n",
    "\n",
    "#         for loc in tqdm(cxn.execute(\"\"\"select * from raw\"\"\")):\n",
    "#             loc = loc[\"v_locality\"]\n",
    "\n",
    "#             try:\n",
    "#                 # Replace HTML entities\n",
    "#                 loc = html.unescape(loc)\n",
    "\n",
    "#                 # Remove control characters & some punct\n",
    "#                 loc = [\" \" if uni.category(c) in remove else c for c in loc]\n",
    "#                 loc = \"\".join(loc)\n",
    "\n",
    "#                 # Normalize chars to ASCII\n",
    "#                 loc = uni.normalize(\"NFKD\", loc)\n",
    "\n",
    "#                 # Lower case the string\n",
    "#                 loc = loc.lower()\n",
    "\n",
    "#                 # Sort words\n",
    "#                 loc = \" \".join(sorted(set(loc.split())))\n",
    "\n",
    "#                 # Too short\n",
    "#                 if len(loc) == 0:\n",
    "#                     raise ValueError\n",
    "\n",
    "#                 # Add it\n",
    "#                 facets[loc] += 1\n",
    "\n",
    "#             except (ValueError, TypeError):\n",
    "#                 errors += 1\n",
    "#                 continue\n",
    "\n",
    "#         batch = [{\"facet\": k, \"hits\": v} for k, v in facets.items()]\n",
    "#         df = pd.DataFrame(batch)\n",
    "#         df.to_sql(\"fingerprints\", cxn, index=False, if_exists=\"replace\")\n",
    "\n",
    "#     return errors\n",
    "\n",
    "\n",
    "# get_fingerprints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1905ff-2574-4beb-ac4d-bc09fae63e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
