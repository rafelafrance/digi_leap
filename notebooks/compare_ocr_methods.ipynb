{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1d91f3-a0c5-4ee2-926e-2091aa6a374e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Measure contributions of parts of OCR pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5b980-67c5-41e7-b49e-1bab3d17d789",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Possible OCR pipelines (actions are green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03270b28-13a7-4cd9-8f66-a7d8ebc0853a",
   "metadata": {},
   "source": [
    "![ocr_flow](assets/ocr_flow.drawio.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a51cd-2831-40b0-b316-81c9926168c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The are 4 groups of actions in the full pipeline (green boxes). The purpose of this notebook is to test if all of these actions really helps with the OCR results, and if they do, by how much.\n",
    "\n",
    "- **Find the single best ensemble.**\n",
    "- Which of the 4 image processing pipelines improve OCR performance?\n",
    "- Two OCR engines: `Tesseract` & `EasyOCR`. `Tesseract` is the current leader in open source OCR engines, does adding `EasyOCR` improve the results?\n",
    "- The `combine text` function is only needed if we stick with the ensemble approach. I.e. only if we use more than one image processing pipeline or more than one OCR engine.\n",
    "- The `clean text` function corrects misspellings and common OCR errors with punctuation, spacing, etc. We want to measure its efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf0268-53f0-4272-b002-e4d67a04c898",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c7d10-a9f7-403e-80b7-39cd294129e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We're doing ablations on the OCR pipeline.\n",
    "- How well do `Tesseract` and `EasyOCR` perform on their own without image pre-processing. I'll also try the engine directly grafted to the `clean text` function.\n",
    "- How well do each of the image pre-processing steps help the OCR process? and which ones work well with which OCR engine. I'm going to try various permutations of these.\n",
    "- Can I whittle this down to one or zero image pre-processing pipelines and one OCR engine? If so, then this would allow me to drop the `combine text` step.\n",
    "- How much does the `clean text` step help?\n",
    "- Note that `EasyOCR` uses a fair bit of GPU resources and if we can remove it it will speed up the OCR pipeline significantly. I.e. `EasyOCR` is difficult to parallelize.\n",
    "\n",
    "#### Scoring\n",
    "- I'll use an expert derived gold standard to compare against the ablation sequences.\n",
    "- I am using Levenshtein distance as the scores. Levenshtein distance counts character mismatches between sequences in a best case pairwise alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2158239-7ae2-4812-b2ed-9e5cc63083e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e14a709-4d3f-475d-aac3-05f11b996f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import display, HTML\n",
    "from collections import defaultdict, namedtuple\n",
    "from datetime import datetime\n",
    "from itertools import groupby\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import jinja2\n",
    "import pandas as pd\n",
    "from colorama import Back, Fore, Style\n",
    "\n",
    "from digi_leap.pylib import consts\n",
    "from digi_leap.pylib.ocr import ocr_compare as compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b0edf4-cd7f-42a1-a901-310fb7b0f294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ec813d-d7e5-4ab0-8dd4-6b839268727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_STD_PATH = consts.DATA_DIR / \"sernec\" / \"gold_std_2022-06-28\"\n",
    "\n",
    "ARGS = SimpleNamespace(\n",
    "    database=consts.DATA_DIR / \"sernec\" / \"sernec.sqlite\",\n",
    "    gold_set=\"gold_set_2022-06-28\",\n",
    "    score_set=\"scores_2022-06-28\",\n",
    "    char_set=\"default\",\n",
    "    notes=\"\",\n",
    "    csv_path=GOLD_STD_PATH / \"gold_std_2022-06-28.csv\",\n",
    "    out_file=consts.DATA_DIR / \"output\" / \"compare_pipelines_2022-08-14.html\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05affe5-8c3f-4efe-bbdc-ba2faac6098e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce5caba-a91e-4b62-a2c3-bf15410f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a new gold standard to a database\n",
    "\n",
    "# compare.insert_gold_std(ARGS.csv_path, ARGS.database, ARGS.gold_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9f7fc4-afe7-45d5-977a-d6dc2df81727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a gold standard from the database\n",
    "\n",
    "GOLD_STD = compare.select_gold_std(ARGS.database, ARGS.gold_set)\n",
    "GOLD_DICT = {g[\"gold_id\"]: g for g in GOLD_STD}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f418c1-45ad-4a68-9b7d-60e80a8945b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OCR scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89bbc4f9-707a-42ef-8288-ec84d8116e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = compare.Scorer(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f41ae18-427f-4fc9-9008-55bf19a5bac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████████████████████████████████▎                                                                        | 162/337 [1:04:27<1:19:42, 27.33s/it]"
     ]
    }
   ],
   "source": [
    "# Calculate new scores\n",
    "\n",
    "SCORES = scorer.calculate(GOLD_STD)\n",
    "scorer.insert_scores(SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11a33a-a4d3-496f-8d34-0b0d931a9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES = scorer.select_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd77b7-b28e-42a6-9ac7-7f7960ec4852",
   "metadata": {},
   "source": [
    "## Examine scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7959e-6b22-4ad5-a7e2-a1df677ecd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek_scores(scores):\n",
    "    grouped_scores = groupby(scores, key=lambda s: s[\"label_id\"])\n",
    "\n",
    "    for labels_id, scores in grouped_scores:\n",
    "        print(\"=\" * 80)\n",
    "        print(labels_id)\n",
    "        scores = sorted(\n",
    "            scores, key=lambda s: (s[\"score\"], len(json.loads(s[\"actions\"])))\n",
    "        )\n",
    "        for score in scores:\n",
    "            print(f\"{score['score']:4d}  {score['actions']}\")\n",
    "\n",
    "\n",
    "# peek_scores(SCORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd408d7d-af0c-43ec-8c3f-07d0eb627b83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def msa_top_scores(scores, gold_std, line_align):\n",
    "    grouped_scores = groupby(scores, key=lambda s: s[\"label_id\"])\n",
    "\n",
    "    for label_id, scores in grouped_scores:\n",
    "        scores = list(scores)\n",
    "        gold = gold_std[scores[0][\"gold_id\"]]\n",
    "        min_score = min(s[\"score\"] for s in scores)\n",
    "        top = [gold[\"gold_text\"]]\n",
    "        top += [s[\"score_text\"] for s in scores if s[\"score\"] == min_score]\n",
    "        top = [\" \".join(ln.split()) for ln in top]\n",
    "\n",
    "        print(f\"{label_id}  {min_score}\")\n",
    "\n",
    "        aligned = line_align.align(top)\n",
    "\n",
    "        rows = len(aligned)\n",
    "        cols = len(aligned[0])\n",
    "        colored = [list(a) for a in aligned]\n",
    "\n",
    "        for col in range(cols):\n",
    "            col_chars = [aligned[row][col] for row in range(rows)]\n",
    "            if len(set(col_chars)) > 1:\n",
    "                for row in range(rows):\n",
    "                    colored[row][col] = (\n",
    "                        Back.LIGHTGREEN_EX\n",
    "                        + Fore.WHITE\n",
    "                        + colored[row][col]\n",
    "                        + Style.RESET_ALL\n",
    "                    )\n",
    "\n",
    "        # for ln in colored:\n",
    "        #     print(\"\".join(ln))\n",
    "        # print()\n",
    "\n",
    "\n",
    "# msa_top_scores(SCORES, GOLD_DICT, scorer.line_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e15f03-0307-4705-84c3-ee3904a114bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelineScore = namedtuple(\"PipelineScore\", \"score pipeline\")\n",
    "\n",
    "\n",
    "def scores_by_pipeline(scores, gold_std):\n",
    "    tally = defaultdict(int)\n",
    "\n",
    "    for score in scores:\n",
    "        tally[score[\"actions\"]] += score[\"score\"]\n",
    "\n",
    "    tally = [(v, len(a), a) for k, v in tally.items() if (a := json.loads(k))]\n",
    "    tally = sorted(tally)\n",
    "    return [PipelineScore(t[0], t[2]) for t in tally]\n",
    "\n",
    "\n",
    "summed = scores_by_pipeline(SCORES, GOLD_DICT)\n",
    "\n",
    "for sum_ in summed:\n",
    "    print(sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154b39-0b53-4e6b-9619-5b70f2795751",
   "metadata": {},
   "source": [
    "## Output raw scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd44fb71-bdde-491b-9b4e-099fbf7d45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_top_scores(scores, gold_std, line_align):\n",
    "    grouped_scores = groupby(scores, key=lambda s: s[\"label_id\"])\n",
    "\n",
    "    for label_id, scores in grouped_scores:\n",
    "        scores = list(scores)\n",
    "\n",
    "        gold = gold_std[scores[0][\"gold_id\"]]\n",
    "\n",
    "        min_score = min(s[\"score\"] for s in scores)\n",
    "        top = [gold[\"gold_text\"]]\n",
    "        top += [s[\"score_text\"] for s in scores if s[\"score\"] == min_score]\n",
    "        top = [\" \".join(ln.split()) for ln in top]\n",
    "\n",
    "        actions = [\"gold\"] + [s[\"actions\"] for s in scores if s[\"score\"] == min_score]\n",
    "\n",
    "        # print(f\"{label_id}  {min_score}\")\n",
    "\n",
    "        aligned = line_align.align(top)\n",
    "\n",
    "        rows = len(aligned)\n",
    "        cols = len(aligned[0])\n",
    "        colored = [list(a) for a in aligned]\n",
    "\n",
    "        for col in range(cols):\n",
    "            col_chars = [aligned[row][col] for row in range(rows)]\n",
    "            if len(set(col_chars)) > 1:\n",
    "                for row in range(rows):\n",
    "                    colored[row][\n",
    "                        col\n",
    "                    ] = f'<span class=\"green\">{aligned[row][col]}</span>'\n",
    "\n",
    "        colored = [\"\".join(ln) for ln in colored]\n",
    "        gold[\"colored\"] = list(zip(actions, colored))\n",
    "        gold[\"min_score\"] = min_score\n",
    "        gold[\"path\"] = f\"lb_{gold['sheet_id']:06d}_{gold['label_id']:08d}.jpg\"\n",
    "\n",
    "\n",
    "html_top_scores(SCORES, GOLD_DICT, scorer.line_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98372a4d-aeef-44c1-8337-47901d571b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html(gold_std):\n",
    "    env = jinja2.Environment(\n",
    "        loader=jinja2.FileSystemLoader(\"assets/\"),\n",
    "        autoescape=True,\n",
    "    )\n",
    "\n",
    "    template = env.get_template(\"html_template.html\").render(\n",
    "        now=datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M\"),\n",
    "        golden=gold_std,\n",
    "    )\n",
    "\n",
    "    with open(ARGS.out_file, \"w\", encoding=\"utf_8\") as html_file:\n",
    "        html_file.write(template)\n",
    "\n",
    "\n",
    "write_html(GOLD_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f300940-62ce-4594-9bd7-dd76281a597c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
