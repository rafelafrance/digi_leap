{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1d91f3-a0c5-4ee2-926e-2091aa6a374e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Measure contributions of parts of OCR pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5b980-67c5-41e7-b49e-1bab3d17d789",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Possible OCR pipelines (actions are green)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03270b28-13a7-4cd9-8f66-a7d8ebc0853a",
   "metadata": {},
   "source": [
    "![ocr_flow](assets/ocr_flow.drawio.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a51cd-2831-40b0-b316-81c9926168c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The are 4 groups of actions in the full pipeline (green boxes). The purpose of this notebook is to test if all of these pipelines really helps with the OCR results, and if they do, by how much.\n",
    "\n",
    "- **Find the single best ensemble.**\n",
    "- Which of the 4 image processing pipelines improve OCR performance?\n",
    "- Two OCR engines: `Tesseract` & `EasyOCR`. `Tesseract` is the current leader in open source OCR engines, does adding `EasyOCR` improve the results?\n",
    "- The `combine text` function is only needed if we stick with the ensemble approach. I.e. only if we use more than one image processing pipeline or more than one OCR engine.\n",
    "- The `clean text` function corrects misspellings and common OCR errors with punctuation, spacing, etc. We want to measure its efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf0268-53f0-4272-b002-e4d67a04c898",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c7d10-a9f7-403e-80b7-39cd294129e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We're doing ablations on the OCR pipeline.\n",
    "- How well do `Tesseract` and `EasyOCR` perform on their own without image pre-processing. I'll also try the engine directly grafted to the `clean text` function.\n",
    "- How well do each of the image pre-processing steps help the OCR process? and which ones work well with which OCR engine. I'm going to try various permutations of these.\n",
    "- Can I whittle this down to one or zero image pre-processing pipelines and one OCR engine? If so, then this would allow me to drop the `combine text` step.\n",
    "- How much does the `clean text` step help?\n",
    "- Note that `EasyOCR` uses a fair bit of GPU resources and if we can remove it it will speed up the OCR pipeline significantly. I.e. `EasyOCR` is difficult to parallelize.\n",
    "\n",
    "#### Scoring\n",
    "- I'll use an expert derived gold standard to compare against the ablation sequences.\n",
    "- I am using Levenshtein distance as the scores. Levenshtein distance counts character mismatches between sequences in a best case pairwise alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2158239-7ae2-4812-b2ed-9e5cc63083e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e14a709-4d3f-475d-aac3-05f11b996f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from itertools import groupby\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import jinja2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "from digi_leap.pylib import consts\n",
    "from digi_leap.pylib.db import db\n",
    "from digi_leap.pylib.ocr import ocr_compare as compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e01902-2370-464e-b84a-64ab75951345",
   "metadata": {},
   "source": [
    "Keep alignments on one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b0edf4-cd7f-42a1-a901-310fb7b0f294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ec813d-d7e5-4ab0-8dd4-6b839268727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_STD_PATH = consts.DATA_DIR / \"sernec\" / \"gold_std_2022-06-28\"\n",
    "\n",
    "ARGS = SimpleNamespace(\n",
    "    database=consts.DATA_DIR / \"sernec\" / \"sernec.sqlite\",\n",
    "    gold_set=\"gold_set_2022-06-28\",\n",
    "    score_set=\"filter_2022-08-25\",\n",
    "    char_set=\"default\",\n",
    "    notes=\"\",\n",
    "    csv_path=GOLD_STD_PATH / \"gold_std_2022-06-28.csv\",\n",
    "    out_file=consts.DATA_DIR / \"output\" / \"compare_pipelines_2022-08-25.html\",\n",
    "    cache=consts.DATA_DIR / \"temp\" / \"ocr_filtered.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05affe5-8c3f-4efe-bbdc-ba2faac6098e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce5caba-a91e-4b62-a2c3-bf15410f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a new gold standard to a database\n",
    "\n",
    "# compare.insert_gold_std(ARGS.csv_path, ARGS.database, ARGS.gold_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9f7fc4-afe7-45d5-977a-d6dc2df81727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a gold standard from the database\n",
    "\n",
    "GOLD_STD = compare.select_gold_std(ARGS.database, ARGS.gold_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f418c1-45ad-4a68-9b7d-60e80a8945b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## OCR scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbe028-92b9-4a25-9b37-a305ce5eedb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ocr:  43%|██████████████████████████████████████████████████████████████████▋                                                                                        | 145/337 [33:22<35:35, 11.12s/it]"
     ]
    }
   ],
   "source": [
    "OCR = compare.ocr(GOLD_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eeb1b-e407-4b3a-98c4-c3c403a80270",
   "metadata": {},
   "source": [
    "## Cache the OCR scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a7df3-8063-4a3a-8841-9f2eca85d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_failsafe():\n",
    "    as_json = deepcopy(OCR)\n",
    "\n",
    "    with open(ARGS.out_gold, \"w\") as out_file:\n",
    "        json.dump(as_json, out_file, indent=2)\n",
    "\n",
    "\n",
    "write_failsafe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3a04b-dd1c-4449-a285-c720fbe44cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_failsafe():\n",
    "    with open(ARGS.out_gold) as in_file:\n",
    "        ocr = json.load(in_file)\n",
    "\n",
    "    return ocr\n",
    "\n",
    "\n",
    "# OCR = read_failsafe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3176597-0491-4296-893e-948587047037",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES = compare.score(OCR, ARGS.score_set, ARGS.gold_set, processes=8)\n",
    "compare.insert_scores(SCORES, ARGS.database, ARGS.score_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11a33a-a4d3-496f-8d34-0b0d931a9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORES = compare.select_scores(ARGS.database, ARGS.score_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7f291-5e6c-401e-94d5-45a055ea4117",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = [\n",
    "    (\n",
    "        [\"\", \"easyocr\"],\n",
    "        \"-e\",\n",
    "    ),\n",
    "    (\n",
    "        [\"\", \"tesseract\"],\n",
    "        \"-t\",\n",
    "    ),\n",
    "    (\n",
    "        [\"deskew\", \"easyocr\"],\n",
    "        \"De\",\n",
    "    ),\n",
    "    (\n",
    "        [\"deskew\", \"tesseract\"],\n",
    "        \"Dt\",\n",
    "    ),\n",
    "    (\n",
    "        [\"binarize\", \"easyocr\"],\n",
    "        \"Be\",\n",
    "    ),\n",
    "    (\n",
    "        [\"binarize\", \"tesseract\"],\n",
    "        \"Bt\",\n",
    "    ),\n",
    "    (\n",
    "        [\"denoise\", \"easyocr\"],\n",
    "        \"Ne\",\n",
    "    ),\n",
    "    (\n",
    "        [\"denoise\", \"tesseract\"],\n",
    "        \"Nt\",\n",
    "    ),\n",
    "    (\n",
    "        [\"post_process\"],\n",
    "        \"p-\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def encode_actions(actions):\n",
    "    acts = json.loads(actions)\n",
    "    pipe = [c if a in acts else \"__\" for a, c in ACTIONS]\n",
    "    return \"_\".join(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f23a6-ad8a-4804-a55a-5bab3ecf92a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, action):\n",
    "        self.actions: str = action\n",
    "        self.scores: list(float) = []\n",
    "        self.counts = None\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        encoded = encode_actions(self.actions)\n",
    "        return consts.DATA_DIR / \"output\" / \"graphs\" / f\"{encoded}.svg\"\n",
    "\n",
    "    @property\n",
    "    def total_score(self):\n",
    "        return sum(self.scores)\n",
    "\n",
    "    @property\n",
    "    def points(self):\n",
    "        xs, ys = [], []\n",
    "        for errors, times in self.counts:\n",
    "            xs.append(errors)\n",
    "            ys.append(times)\n",
    "        return xs, ys\n",
    "\n",
    "    def add_score(self, score, gold_len):\n",
    "        self.scores.append(round(score / gold_len * 100.0, 2))\n",
    "\n",
    "    def calc_counts(self):\n",
    "        counts = Counter(self.scores)\n",
    "        self.counts = sorted(counts.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61a689-dcc5-4d74-a80e-027938f11579",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelinesDict(dict):\n",
    "    def __missing__(self, action):\n",
    "        pipeline = self[action] = Pipeline(action)\n",
    "        return pipeline\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(self.values(), key=lambda v: v.total_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add74ae-583f-4763-a4ad-96cdac4d31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_violins():\n",
    "    sql = \"\"\"select actions, score, length(gold_text) as gold_len\n",
    "               from ocr_scores\n",
    "               join gold_standard using (gold_id)\n",
    "              where score_set = ?\n",
    "                and ocr_scores.gold_set = ?\"\"\"\n",
    "    with db.connect(ARGS.database) as cxn:\n",
    "        scores = db.execute(cxn, sql, [ARGS.score_set, ARGS.gold_set])\n",
    "\n",
    "    pipeline_dict = PipelinesDict()\n",
    "\n",
    "    for score in scores:\n",
    "        pipeline_dict[score[\"actions\"]].add_score(score[\"score\"], score[\"gold_len\"])\n",
    "\n",
    "    height = 10\n",
    "    rows = 9\n",
    "    cols = 10\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(40, height * rows))\n",
    "\n",
    "    for i, pipe in enumerate(pipeline_dict):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        axes[r, c].violinplot(\n",
    "            pipe.scores,\n",
    "            showextrema=True,\n",
    "            showmedians=True,\n",
    "            showmeans=True,\n",
    "        )\n",
    "        mx = round(max(pipe.scores))\n",
    "        mn = round(min(pipe.scores))\n",
    "        title = encode_actions(pipe.actions)\n",
    "        title = title.strip(\"_\")\n",
    "        title = re.sub(r\"__+\", \"_\", title)\n",
    "        title = re.sub(r\"-\", \"\", title)\n",
    "        axes[r, c].set_title(title)\n",
    "        axes[r, c].set_ylabel(f\"errors ({mn} - {mx})\")\n",
    "        axes[r, c].set_xticklabels([])\n",
    "\n",
    "    path = consts.DATA_DIR / \"output\" / \"graphs\" / \"violinplot.jpg\"\n",
    "    plt.savefig(path, format=\"jpg\")\n",
    "    plt.show()\n",
    "\n",
    "    return pipeline_dict\n",
    "\n",
    "\n",
    "# pipeline_violins = get_violins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f140ddf-d60f-4a8c-9c7b-2c847df06b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes():\n",
    "    sql = \"\"\"select actions, score, length(gold_text) as gold_len\n",
    "               from ocr_scores\n",
    "               join gold_standard using (gold_id)\n",
    "              where score_set = ?\n",
    "                and ocr_scores.gold_set = ?\"\"\"\n",
    "    with db.connect(ARGS.database) as cxn:\n",
    "        scores = db.execute(cxn, sql, [ARGS.score_set, ARGS.gold_set])\n",
    "\n",
    "    pipeline_dict = PipelinesDict()\n",
    "\n",
    "    for score in scores:\n",
    "        pipeline_dict[score[\"actions\"]].add_score(score[\"score\"], score[\"gold_len\"])\n",
    "\n",
    "    height = 10\n",
    "    rows = 9\n",
    "    cols = 10\n",
    "    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(40, height * rows))\n",
    "\n",
    "    for i, pipe in enumerate(pipeline_dict):\n",
    "        r = i // cols\n",
    "        c = i % cols\n",
    "        axes[r, c].boxplot(\n",
    "            pipe.scores,\n",
    "        )\n",
    "        mx = round(max(pipe.scores))\n",
    "        mn = round(min(pipe.scores))\n",
    "        title = encode_actions(pipe.actions)\n",
    "        title = title.strip(\"_\")\n",
    "        title = re.sub(r\"__+\", \"_\", title)\n",
    "        title = re.sub(r\"-\", \"\", title)\n",
    "        axes[r, c].set_title(title)\n",
    "        axes[r, c].set_ylabel(f\"errors ({mn} - {mx})\")\n",
    "        axes[r, c].set_xticklabels([])\n",
    "\n",
    "    path = consts.DATA_DIR / \"output\" / \"graphs\" / \"boxplot.jpg\"\n",
    "    plt.savefig(path, format=\"jpg\")\n",
    "    plt.show()\n",
    "\n",
    "    return pipeline_dict\n",
    "\n",
    "\n",
    "# pipeline_boxes = get_boxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcd77b7-b28e-42a6-9ac7-7f7960ec4852",
   "metadata": {},
   "source": [
    "## Total scores by pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e15f03-0307-4705-84c3-ee3904a114bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelineScore = namedtuple(\"PipelineScore\", \"score pipeline\")\n",
    "\n",
    "\n",
    "def scores_by_pipeline(scores):\n",
    "    tally = defaultdict(int)\n",
    "\n",
    "    for score in scores:\n",
    "        tally[score[\"pipeline\"]] += score[\"score\"]\n",
    "\n",
    "    tally = [(v, len(k), k) for k, v in tally.items()]\n",
    "    tally = sorted(tally)\n",
    "\n",
    "    return [PipelineScore(t[0], t[2]) for t in tally]\n",
    "\n",
    "\n",
    "summed = scores_by_pipeline(SCORES)\n",
    "for sum_ in summed:\n",
    "    print(sum_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884c803-d23e-4fce-86d7-efe051c70cbc",
   "metadata": {},
   "source": [
    "## Output how many times a pipeline was the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7a7ab-5efe-4335-964d-296db58bbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelineWinner = namedtuple(\"PipelineWinner\", \"count pipeline\")\n",
    "\n",
    "\n",
    "def winners_by_pipeline(all_scores):\n",
    "    tally = defaultdict(int)\n",
    "    grouped_scores = groupby(all_scores, key=lambda s: s[\"label_id\"])\n",
    "\n",
    "    for label_id, scores in grouped_scores:\n",
    "        scores = list(scores)\n",
    "\n",
    "        min_score = min(s[\"score\"] for s in scores)\n",
    "        for score in scores:\n",
    "            if score[\"score\"] == min_score:\n",
    "                tally[score[\"pipeline\"]] += 1\n",
    "\n",
    "    tally = [(v, -len(k), k) for k, v in tally.items()]\n",
    "    tally = sorted(tally, reverse=True)\n",
    "    return [PipelineWinner(t[0], t[2]) for t in tally]\n",
    "\n",
    "\n",
    "winners = winners_by_pipeline(SCORES)\n",
    "for win in winners:\n",
    "    print(win)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfe66f-c6e0-4c64-81f8-63b8cdd8b4c1",
   "metadata": {},
   "source": [
    "# Distribution of scores per pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154b39-0b53-4e6b-9619-5b70f2795751",
   "metadata": {},
   "source": [
    "## Output best scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de26088-7397-48bc-9fdd-68bc6abfd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_top_scores(scores, gold_std, line_align):\n",
    "    grouped_scores = groupby(scores, key=lambda s: s[\"label_id\"])\n",
    "\n",
    "    for label_id, scores in tqdm(grouped_scores):\n",
    "        scores = list(scores)\n",
    "\n",
    "        gold = gold_std[scores[0][\"gold_id\"]]\n",
    "\n",
    "        min_score = min(s[\"score\"] for s in scores)\n",
    "\n",
    "        counts = defaultdict(int)\n",
    "        for s in scores:\n",
    "            counts[s[\"score\"]] += 1\n",
    "        counts = sorted(counts.items())\n",
    "\n",
    "        plt.bar([c[0] for c in counts], [c[1] for c in counts])\n",
    "        graph = (\n",
    "            consts.DATA_DIR\n",
    "            / \"output\"\n",
    "            / \"graphs\"\n",
    "            / f\"lb_{gold['sheet_id']:06d}_{gold['label_id']:08d}.svg\"\n",
    "        )\n",
    "        plt.savefig(graph, format=\"svg\")\n",
    "        plt.close()\n",
    "\n",
    "        # Shrink actions to codes\n",
    "        actions = [f\"gold {'__ ' * 7}\"]\n",
    "        for acts in [\n",
    "            json.loads(s[\"actions\"]) for s in scores if s[\"score\"] == min_score\n",
    "        ]:\n",
    "            pipe = [c if a in acts else \"__\" for a, c in ACTIONS]\n",
    "            actions.append(\" \".join(pipe))\n",
    "\n",
    "        top = [gold[\"gold_text\"]]\n",
    "        top += [s[\"score_text\"] for s in scores if s[\"score\"] == min_score]\n",
    "\n",
    "        top = [\" \".join(ln.split()) for ln in top]\n",
    "\n",
    "        # print(f\"{label_id}  {min_score}  {len(top) - 1}\")\n",
    "\n",
    "        aligned = line_align.align(top)\n",
    "\n",
    "        rows = len(aligned)\n",
    "        cols = len(aligned[0])\n",
    "        colored = [list(a) for a in aligned]\n",
    "\n",
    "        for col in range(cols):\n",
    "            col_chars = [aligned[row][col] for row in range(rows)]\n",
    "            if len(set(col_chars)) > 1:\n",
    "                for i, row in enumerate(range(rows)):\n",
    "                    if i == 0:\n",
    "                        colored[row][\n",
    "                            col\n",
    "                        ] = f'<span class=\"yellow\">{aligned[row][col]}</span>'\n",
    "                    elif aligned[0][col] != aligned[row][col]:\n",
    "                        colored[row][\n",
    "                            col\n",
    "                        ] = f'<span class=\"green\">{aligned[row][col]}</span>'\n",
    "\n",
    "        colored = [\"\".join(ln) for ln in colored]\n",
    "        gold[\"colored\"] = list(zip(actions, colored))\n",
    "        gold[\"min_score\"] = min_score\n",
    "        gold[\"path\"] = f\"lb_{gold['sheet_id']:06d}_{gold['label_id']:08d}.jpg\"\n",
    "        gold[\"graph\"] = f\"lb_{gold['sheet_id']:06d}_{gold['label_id']:08d}.svg\"\n",
    "\n",
    "\n",
    "# html_top_scores(SCORES, GOLD_DICT, scorer.line_align)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98372a4d-aeef-44c1-8337-47901d571b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_html(gold_std):\n",
    "    env = jinja2.Environment(\n",
    "        loader=jinja2.FileSystemLoader(\"assets/\"),\n",
    "        autoescape=True,\n",
    "    )\n",
    "\n",
    "    template = env.get_template(\"html_template.html\").render(\n",
    "        now=datetime.strftime(datetime.now(), \"%Y-%m-%d %H:%M\"),\n",
    "        golden=gold_std,\n",
    "    )\n",
    "\n",
    "    with open(ARGS.out_file, \"w\", encoding=\"utf_8\") as html_file:\n",
    "        html_file.write(template)\n",
    "\n",
    "\n",
    "# write_html(GOLD_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f300940-62ce-4594-9bd7-dd76281a597c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
